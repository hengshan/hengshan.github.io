"""
æŠ€æœ¯ä¿¡æ¯æºç›‘æ§æ¨¡å—
ç›‘æ§arXivã€GitHubã€æŠ€æœ¯åšå®¢ç­‰è·å–æœ€æ–°æŠ€æœ¯åŠ¨æ€
"""

import requests
import feedparser
from datetime import datetime, timedelta
from typing import List, Dict
import yaml


class TechMonitor:
    """æŠ€æœ¯ä¿¡æ¯æºç›‘æ§å™¨"""

    def __init__(self, sources_config_path: str):
        with open(sources_config_path, 'r', encoding='utf-8') as f:
            self.sources = yaml.safe_load(f)

    def fetch_arxiv_papers(self, max_results: int = 5) -> List[Dict]:
        """ä»arXivè·å–æœ€æ–°è®ºæ–‡"""
        papers = []
        base_url = self.sources['arxiv']['base_url']

        for category_info in self.sources['arxiv']['categories']:
            cat_id = category_info['id']
            keywords = category_info.get('keywords', [])

            # æ„å»ºæŸ¥è¯¢
            query = f"cat:{cat_id}"
            params = {
                'search_query': query,
                'start': 0,
                'max_results': max_results,
                'sortBy': 'submittedDate',
                'sortOrder': 'descending'
            }

            response = requests.get(base_url, params=params)
            feed = feedparser.parse(response.content)

            for entry in feed.entries:
                # æ£€æŸ¥å…³é”®è¯åŒ¹é…
                title_summary = (entry.title + ' ' + entry.summary).lower()
                if any(kw.lower() in title_summary for kw in keywords) or not keywords:
                    papers.append({
                        'source': 'arxiv',
                        'title': entry.title,
                        'summary': entry.summary,
                        'url': entry.link,
                        'published': entry.published,
                        'category': category_info['name'],
                        'keywords': keywords
                    })

        return papers

    def fetch_github_trending(self) -> List[Dict]:
        """è·å–GitHub trendingå’Œé‡è¦æ›´æ–°"""
        trending = []

        for repo_info in self.sources['github']['repos']:
            owner = repo_info['owner']
            name = repo_info['name']

            # GitHub API: è·å–æœ€è¿‘çš„releaseså’Œcommits
            # æ³¨æ„ï¼šéœ€è¦GITHUB_TOKENç¯å¢ƒå˜é‡
            try:
                # è·å–æœ€æ–°release
                url = f"https://api.github.com/repos/{owner}/{name}/releases/latest"
                response = requests.get(url)
                if response.status_code == 200:
                    release = response.json()
                    trending.append({
                        'source': 'github',
                        'repo': f"{owner}/{name}",
                        'title': release['name'],
                        'description': release['body'][:500],  # æˆªå–å‰500å­—ç¬¦
                        'url': release['html_url'],
                        'published': release['published_at'],
                        'type': 'release',
                        'topics': repo_info.get('topics', [])
                    })
            except Exception as e:
                print(f"Error fetching {owner}/{name}: {e}")

        return trending

    def fetch_blog_posts(self) -> List[Dict]:
        """ä»æŠ€æœ¯åšå®¢RSSè·å–æœ€æ–°æ–‡ç« """
        posts = []

        for blog in self.sources.get('blogs', []):
            try:
                feed = feedparser.parse(blog['url'])
                for entry in feed.entries[:5]:  # æ¯ä¸ªåšå®¢å–å‰5ç¯‡
                    # æ£€æŸ¥å…³é”®è¯åŒ¹é…
                    title_summary = (entry.title + ' ' + entry.summary).lower()
                    keywords = blog.get('keywords', [])

                    if any(kw.lower() in title_summary for kw in keywords) or not keywords:
                        posts.append({
                            'source': 'blog',
                            'blog_name': blog['name'],
                            'title': entry.title,
                            'summary': entry.get('summary', '')[:500],
                            'url': entry.link,
                            'published': entry.get('published', ''),
                            'keywords': keywords
                        })
            except Exception as e:
                print(f"Error fetching {blog['name']}: {e}")

        return posts

    def fetch_openreview_papers(self, max_results: int = 10) -> List[Dict]:
        """ä»OpenReviewè·å–æœ€æ–°è®ºæ–‡"""
        papers = []

        if 'openreview' not in self.sources:
            return papers

        try:
            api_url = self.sources['openreview'].get('api_url', 'https://api.openreview.net/notes')
            limit = self.sources['openreview'].get('limit', 10)

            for venue_info in self.sources['openreview'].get('venues', []):
                venue_id = venue_info['id']
                keywords = venue_info.get('keywords', [])

                # æŸ¥è¯¢è¯¥ä¼šè®®çš„è®ºæ–‡
                params = {
                    'invitation': venue_id + '/-/Blind_Submission',
                    'details': 'replyCount,writable',
                    'limit': limit
                }

                response = requests.get(api_url, params=params, timeout=10)

                if response.status_code == 200:
                    data = response.json()
                    notes = data.get('notes', [])

                    for note in notes[:max_results]:
                        content = note.get('content', {})
                        title = content.get('title', 'No title')
                        abstract = content.get('abstract', '')

                        # æ£€æŸ¥å…³é”®è¯åŒ¹é…ï¼ˆå¦‚æœæœ‰ï¼‰
                        text = (title + ' ' + abstract).lower()
                        if keywords and not any(kw.lower() in text for kw in keywords):
                            continue

                        papers.append({
                            'source': 'openreview',
                            'venue': venue_info['name'],
                            'title': title,
                            'summary': abstract[:500] if abstract else '',
                            'url': f"https://openreview.net/forum?id={note.get('id', '')}",
                            'published': note.get('tmdate', '')[:10],
                            'keywords': keywords
                        })

        except Exception as e:
            print(f"Error fetching OpenReview: {e}")

        return papers

    def aggregate_and_rank(self, papers: List[Dict], trending: List[Dict],
                          posts: List[Dict], openreview_papers: List[Dict],
                          category_weights: Dict, target_category: str = None) -> List[Dict]:
        """èšåˆæ‰€æœ‰ä¿¡æ¯å¹¶æ ¹æ®ç”¨æˆ·åå¥½æ’åº

        Args:
            papers: arXivè®ºæ–‡åˆ—è¡¨
            trending: GitHub trendingåˆ—è¡¨
            posts: åšå®¢æ–‡ç« åˆ—è¡¨
            openreview_papers: OpenReviewè®ºæ–‡åˆ—è¡¨
            category_weights: ç±»åˆ«æƒé‡å­—å…¸
            target_category: ç›®æ ‡ç±»åˆ«ï¼Œå¦‚æœæŒ‡å®šåˆ™ç»™åŒ¹é…çš„å†…å®¹å¤§å¹…åŠ åˆ†
        """
        all_items = papers + trending + posts + openreview_papers

        # ç±»åˆ«å…³é”®è¯æ˜ å°„
        category_keywords = {
            'CUDA/GPUç¼–ç¨‹': ['cuda', 'gpu', 'kernel', 'tensor core', 'tensorcore', 'nvidia', 'gpgpu'],
            'ML/DLç®—æ³•å®ç°': ['transformer', 'attention', 'neural', 'deep learning', 'llm', 'bert', 'gpt'],
            'å¼ºåŒ–å­¦ä¹ ': ['reinforcement', 'rl', 'ppo', 'dqn', 'policy', 'reward', 'agent', 'mdp'],
            'æ¨ç†ä¼˜åŒ–': ['inference', 'flash attention', 'quantization', 'pruning', 'distillation', 'tensorrt'],
            'ä¼˜åŒ–ä¸ç§‘å­¦è®¡ç®—': ['optimization', 'gradient', 'convex', 'numerical', 'solver', 'linear algebra'],
            'Spatial Intelligence': ['nerf', 'gaussian splatting', '3d reconstruction', 'slam', 'point cloud',
                                     'spatial', '3d vision', 'depth estimation', 'pose estimation', 'voxel',
                                     'mesh', 'geometry', 'camera', 'lidar', 'rgbd', '3d gaussian', 'radiance field',
                                     'scene reconstruction', 'view synthesis', 'stereo', 'multiview']
        }

        # ç®€å•è¯„åˆ†ç³»ç»Ÿ
        for item in all_items:
            score = 0

            # æ ¹æ®åˆ†ç±»æƒé‡è¯„åˆ†
            item_text = (item.get('title', '') + ' ' +
                        item.get('summary', '') + ' ' +
                        item.get('description', '') + ' ' +
                        str(item.get('keywords', [])) + ' ' +
                        str(item.get('topics', []))).lower()

            # è®°å½•åŒ¹é…çš„ç±»åˆ«
            matched_categories = []

            for category, keywords in category_keywords.items():
                if any(kw in item_text for kw in keywords):
                    weight = category_weights.get(category, 0.1)
                    score += weight * 10
                    matched_categories.append(category)

            # å¦‚æœæŒ‡å®šäº†ç›®æ ‡ç±»åˆ«ï¼Œç»™åŒ¹é…çš„å†…å®¹å¤§å¹…åŠ åˆ†
            if target_category:
                target_keywords = category_keywords.get(target_category, [])
                if any(kw in item_text for kw in target_keywords):
                    score += 50  # ç›®æ ‡ç±»åˆ«åŒ¹é…åŠ 50åˆ†
                    item['matched_target'] = True
                else:
                    item['matched_target'] = False

            item['matched_categories'] = matched_categories

            # æ—¶æ•ˆæ€§åŠ åˆ†ï¼ˆæœ€è¿‘3å¤©çš„å†…å®¹ï¼‰
            try:
                pub_date = datetime.strptime(item.get('published', '')[:10], '%Y-%m-%d')
                days_old = (datetime.now() - pub_date).days
                if days_old <= 3:
                    score += 5
                elif days_old <= 7:
                    score += 2
            except:
                pass

            item['score'] = score

        # æŒ‰åˆ†æ•°æ’åº
        all_items.sort(key=lambda x: x['score'], reverse=True)

        return all_items

    def get_daily_recommendations(self, category_weights: Dict, top_n: int = 10,
                                    target_category: str = None) -> List[Dict]:
        """è·å–æ¯æ—¥æ¨èæŠ€æœ¯è¯é¢˜

        Args:
            category_weights: ç±»åˆ«æƒé‡å­—å…¸
            top_n: è¿”å›çš„æ¨èæ•°é‡
            target_category: ç›®æ ‡ç±»åˆ«ï¼Œå¦‚æœæŒ‡å®šåˆ™ä¼˜å…ˆæ¨èè¯¥ç±»åˆ«çš„å†…å®¹
        """
        print("ğŸ“¡ æ­£åœ¨ç›‘æ§æŠ€æœ¯ä¿¡æ¯æº...")

        papers = self.fetch_arxiv_papers()
        print(f"  âœ“ ä»arXivè·å–äº† {len(papers)} ç¯‡è®ºæ–‡")

        trending = self.fetch_github_trending()
        print(f"  âœ“ ä»GitHubè·å–äº† {len(trending)} ä¸ªæ›´æ–°")

        posts = self.fetch_blog_posts()
        print(f"  âœ“ ä»æŠ€æœ¯åšå®¢è·å–äº† {len(posts)} ç¯‡æ–‡ç« ")

        openreview_papers = self.fetch_openreview_papers()
        if openreview_papers:
            print(f"  âœ“ ä»OpenReviewè·å–äº† {len(openreview_papers)} ç¯‡è®ºæ–‡")

        ranked = self.aggregate_and_rank(papers, trending, posts, openreview_papers,
                                         category_weights, target_category)

        if target_category:
            print(f"\nğŸ¯ é’ˆå¯¹'{target_category}'ç±»åˆ«ï¼Œæ¨èä»¥ä¸‹ {min(top_n, len(ranked))} ä¸ªè¯é¢˜ï¼š")
        else:
            print(f"\nğŸ¯ æ ¹æ®ä½ çš„åå¥½ï¼Œæ¨èä»¥ä¸‹ {min(top_n, len(ranked))} ä¸ªè¯é¢˜ï¼š")
        for i, item in enumerate(ranked[:top_n], 1):
            print(f"  {i}. [{item['source']}] {item['title'][:60]}... (è¯„åˆ†: {item['score']:.1f})")

        return ranked[:top_n]


if __name__ == "__main__":
    # æµ‹è¯•
    monitor = TechMonitor('../sources/tech_sources.yaml')
    weights = {
        'CUDA/GPUç¼–ç¨‹': 0.3,
        'ML/DLç®—æ³•å®ç°': 0.25,
        'å¼ºåŒ–å­¦ä¹ ': 0.2,
        'æ¨ç†ä¼˜åŒ–': 0.15,
        'ä¼˜åŒ–ä¸ç§‘å­¦è®¡ç®—': 0.1,
        'Spatial Intelligence': 0.2
    }

    print("=" * 60)
    print("æµ‹è¯•1: æ— æŒ‡å®šç±»åˆ«")
    print("=" * 60)
    recommendations = monitor.get_daily_recommendations(weights)

    print("\n" + "=" * 60)
    print("æµ‹è¯•2: æŒ‡å®š Spatial Intelligence ç±»åˆ«")
    print("=" * 60)
    recommendations_spatial = monitor.get_daily_recommendations(
        weights, target_category='Spatial Intelligence'
    )
