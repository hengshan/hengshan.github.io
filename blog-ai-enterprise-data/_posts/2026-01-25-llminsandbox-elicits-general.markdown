---
layout: post-wide
title: "LLM-in-Sandboxå¼ºåŒ–å­¦ä¹ ï¼šè®©è¯­è¨€æ¨¡å‹å­¦ä¼šä½¿ç”¨ä»£ç æ²™ç®±çš„æ™ºèƒ½ä½“è®­ç»ƒ"
date: 2026-01-25 12:34:24 +0800
category: AI
author: Hank Li
use_math: true
source_url: https://arxiv.org/abs/2601.16206v1
generated_by: AI Agent
---

## RLé—®é¢˜è®¾å®š

LLM-in-Sandboxå°†è¯­è¨€æ¨¡å‹åœ¨ä»£ç æ²™ç®±ä¸­çš„æ¢ç´¢è¿‡ç¨‹å»ºæ¨¡ä¸ºä¸€ä¸ªé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼š

- **çŠ¶æ€ç©ºé—´ S**ï¼šå½“å‰å¯¹è¯å†å²ã€æ²™ç®±æ–‡ä»¶ç³»ç»ŸçŠ¶æ€ã€å·²æ‰§è¡Œä»£ç çš„è¾“å‡ºç»“æœ
- **åŠ¨ä½œç©ºé—´ A**ï¼šç”Ÿæˆæ–‡æœ¬å›å¤ã€æ‰§è¡ŒPythonä»£ç ã€è¯»å†™æ–‡ä»¶ã€è°ƒç”¨å¤–éƒ¨å·¥å…·
- **å¥–åŠ±å‡½æ•° R**ï¼šä»»åŠ¡å®Œæˆè´¨é‡ï¼ˆå¦‚ç­”æ¡ˆæ­£ç¡®æ€§ï¼‰ã€æ‰§è¡Œæ•ˆç‡ã€èµ„æºä½¿ç”¨åˆç†æ€§
- **çŠ¶æ€è½¬ç§» P**ï¼šç”±ç”¨æˆ·è¾“å…¥å’Œä»£ç æ‰§è¡Œç»“æœå†³å®šçš„ç¡®å®šæ€§è½¬ç§»

è¿™æ˜¯ä¸€ä¸ª**on-policyå¼ºåŒ–å­¦ä¹ é—®é¢˜**ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦è®­ç»ƒæ¨¡å‹åœ¨äº¤äº’è¿‡ç¨‹ä¸­å®æ—¶åšå‡ºå†³ç­–ã€‚ä¸ä¼ ç»ŸRLç®—æ³•ä¸åŒï¼Œè¯¥é—®é¢˜çš„ç‰¹æ®Šæ€§åœ¨äºï¼š
1. åŠ¨ä½œç©ºé—´æ˜¯ç¦»æ•£çš„é«˜ç»´æ–‡æœ¬åºåˆ—ï¼ˆtokençº§åˆ«ï¼‰
2. å¥–åŠ±é€šå¸¸æ˜¯ç¨€ç–çš„ï¼ˆä»…åœ¨ä»»åŠ¡ç»“æŸæ—¶è·å¾—ï¼‰
3. éœ€è¦å¤„ç†é•¿æœŸä¾èµ–ï¼ˆmulti-step reasoningï¼‰

è¯¥æ–¹æ³•ç»“åˆäº†**è¡Œä¸ºå…‹éš†ï¼ˆBehavioral Cloningï¼‰**å’Œ**å¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼ˆRLHFï¼‰**ï¼Œä½¿ç”¨éæ™ºèƒ½ä½“æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå´èƒ½æ¿€å‘å‡ºæ™ºèƒ½ä½“è¡Œä¸ºã€‚

## ç®—æ³•åŸç†

### æ ¸å¿ƒåˆ›æ–°ï¼šä»éæ™ºèƒ½ä½“æ•°æ®ä¸­å­¦ä¹ æ™ºèƒ½ä½“è¡Œä¸º

ä¼ ç»Ÿæ–¹æ³•éœ€è¦å¤§é‡çš„æ™ºèƒ½ä½“è½¨è¿¹æ•°æ®ï¼ˆåŒ…å«å·¥å…·è°ƒç”¨ã€ä»£ç æ‰§è¡Œç­‰ï¼‰ï¼Œè€ŒLLM-in-Sandbox-RLçš„åˆ›æ–°åœ¨äºï¼š

**æ•°å­¦æ¨å¯¼**ï¼š

å®šä¹‰ç­–ç•¥ $\pi_\theta(a|s)$ ä¸ºç»™å®šçŠ¶æ€ $s$ ä¸‹é€‰æ‹©åŠ¨ä½œ $a$ çš„æ¦‚ç‡ã€‚ä¼˜åŒ–ç›®æ ‡æ˜¯æœ€å¤§åŒ–æœŸæœ›ç´¯ç§¯å¥–åŠ±ï¼š

$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \gamma^t R(s_t, a_t) \right]
$$

ä½¿ç”¨ç­–ç•¥æ¢¯åº¦å®šç†ï¼š

$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) A^{\pi_\theta}(s_t, a_t) \right]
$$

å…¶ä¸­ $A^{\pi_\theta}(s_t, a_t)$ æ˜¯ä¼˜åŠ¿å‡½æ•°ï¼ˆAdvantage Functionï¼‰ã€‚

**å…³é”®åˆ›æ–°**ï¼šä½¿ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥

1. **é˜¶æ®µä¸€ï¼šSandbox-Aware Pretraining**
   - åœ¨éæ™ºèƒ½ä½“æ•°æ®ä¸Šè®­ç»ƒï¼Œä½†å¢å¼ºæ²™ç®±æ„ŸçŸ¥èƒ½åŠ›
   - ä½¿ç”¨æ•°æ®å¢å¼ºï¼šå°†æ™®é€šQAè½¬æ¢ä¸º"å¯èƒ½éœ€è¦æ²™ç®±"çš„æ ¼å¼

2. **é˜¶æ®µäºŒï¼šExploration RL**
   - ä½¿ç”¨PPOç®—æ³•è¿›è¡Œåœ¨çº¿æ¢ç´¢
   - å¥–åŠ±å¡‘å½¢ï¼ˆReward Shapingï¼‰ï¼šä¸­é—´æ­¥éª¤å¥–åŠ± + æœ€ç»ˆä»»åŠ¡å¥–åŠ±

**ç®—æ³•ä¼ªä»£ç **ï¼š

```
Algorithm: LLM-in-Sandbox-RL

Input: Base LLM Ï€â‚€, Task distribution D, Sandbox environment E
Output: Fine-tuned policy Ï€*

# é˜¶æ®µä¸€ï¼šSandbox-Aware Pretraining
for epoch in pretraining_epochs:
    for batch in non_agentic_data:
        # æ•°æ®å¢å¼ºï¼šæ³¨å…¥æ²™ç®±æç¤º
        augmented_batch = add_sandbox_hints(batch)
        # ç›‘ç£å­¦ä¹ 
        loss = cross_entropy_loss(Ï€_Î¸, augmented_batch)
        update_parameters(Î¸, loss)

# é˜¶æ®µäºŒï¼šExploration RL (PPO)
for iteration in rl_iterations:
    # æ”¶é›†è½¨è¿¹
    trajectories = []
    for task in sample_tasks(D):
        state = E.reset(task)
        trajectory = []
        for step in max_steps:
            action = Ï€_Î¸.sample(state)
            next_state, reward, done = E.step(action)
            trajectory.append((state, action, reward))
            state = next_state
            if done: break
        trajectories.append(trajectory)
    
    # è®¡ç®—ä¼˜åŠ¿å‡½æ•°
    advantages = compute_gae(trajectories, Î³=0.99, Î»=0.95)
    
    # PPOæ›´æ–°
    for ppo_epoch in ppo_epochs:
        for batch in trajectories:
            ratio = Ï€_Î¸(a|s) / Ï€_old(a|s)
            clipped_ratio = clip(ratio, 1-Îµ, 1+Îµ)
            loss = -min(ratio * A, clipped_ratio * A)
            update_parameters(Î¸, loss)
    
    Ï€_old = Ï€_Î¸

return Ï€_Î¸
```

## å®ç°ï¼šç®€å•ç¯å¢ƒ

### ç¯å¢ƒå®šä¹‰

æˆ‘ä»¬é¦–å…ˆå®ç°ä¸€ä¸ªç®€åŒ–çš„æ²™ç®±ç¯å¢ƒï¼Œæ”¯æŒåŸºæœ¬çš„ä»£ç æ‰§è¡Œå’Œæ–‡ä»¶æ“ä½œï¼š

```python
import subprocess
import tempfile
import os
import json
from typing import Dict, Tuple, Any
from dataclasses import dataclass

@dataclass
class SandboxState:
    """æ²™ç®±çŠ¶æ€å®šä¹‰"""
    conversation_history: list  # å¯¹è¯å†å²
    file_system: Dict[str, str]  # æ–‡ä»¶ç³»ç»Ÿ {æ–‡ä»¶å: å†…å®¹}
    last_output: str  # ä¸Šæ¬¡æ‰§è¡Œè¾“å‡º
    task_description: str  # ä»»åŠ¡æè¿°
    
class SimpleSandboxEnv:
    """
    ç®€åŒ–çš„ä»£ç æ²™ç®±ç¯å¢ƒ
    æ”¯æŒPythonä»£ç æ‰§è¡Œã€æ–‡ä»¶è¯»å†™ã€ç®€å•çš„å¥–åŠ±è®¡ç®—
    """
    def __init__(self, max_steps: int = 10, timeout: int = 5):
        self.max_steps = max_steps
        self.timeout = timeout  # ä»£ç æ‰§è¡Œè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        self.current_step = 0
        self.temp_dir = None
        
    def reset(self, task: str) -> SandboxState:
        """é‡ç½®ç¯å¢ƒï¼Œå¼€å§‹æ–°ä»»åŠ¡"""
        self.current_step = 0
        # åˆ›å»ºä¸´æ—¶ç›®å½•ä½œä¸ºæ²™ç®±
        if self.temp_dir:
            self._cleanup_temp_dir()
        self.temp_dir = tempfile.mkdtemp(prefix="sandbox_")
        
        self.state = SandboxState(
            conversation_history=[{"role": "user", "content": task}],
            file_system={},
            last_output="",
            task_description=task
        )
        return self.state
    
    def step(self, action: Dict[str, Any]) -> Tuple[SandboxState, float, bool, Dict]:
        """
        æ‰§è¡Œä¸€æ­¥åŠ¨ä½œ
        actionæ ¼å¼: {"type": "code"|"text", "content": str}
        è¿”å›: (next_state, reward, done, info)
        """
        self.current_step += 1
        reward = 0.0
        done = False
        info = {}
        
        if action["type"] == "code":
            # æ‰§è¡Œä»£ç 
            output, success = self._execute_code(action["content"])
            self.state.last_output = output
            
            # å¥–åŠ±å¡‘å½¢ï¼šæˆåŠŸæ‰§è¡Œç»™äºˆå°å¥–åŠ±
            if success:
                reward += 0.1
            else:
                reward -= 0.1
                
            # æ›´æ–°å¯¹è¯å†å²
            self.state.conversation_history.append({
                "role": "assistant",
                "content": f"```python\n{action['content']}\n```"
            })
            self.state.conversation_history.append({
                "role": "system",
                "content": f"Output: {output}"
            })
            
        elif action["type"] == "text":
            # æ–‡æœ¬å›å¤
            self.state.conversation_history.append({
                "role": "assistant",
                "content": action["content"]
            })
            
        elif action["type"] == "submit":
            # æäº¤ç­”æ¡ˆ
            done = True
            # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è®¡ç®—æœ€ç»ˆå¥–åŠ±
            # ç¤ºä¾‹ï¼šç®€å•çš„å­—ç¬¦ä¸²åŒ¹é…
            if "answer" in action:
                reward = self._compute_final_reward(action["answer"])
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§æ­¥æ•°
        if self.current_step >= self.max_steps:
            done = True
            
        info = {
            "step": self.current_step,
            "success": reward > 0 if done else None
        }
        
        return self.state, reward, done, info
    
    def _execute_code(self, code: str) -> Tuple[str, bool]:
        """
        åœ¨æ²™ç®±ä¸­å®‰å…¨æ‰§è¡ŒPythonä»£ç 
        è¿”å›: (è¾“å‡º, æ˜¯å¦æˆåŠŸ)
        """
        # åˆ›å»ºä¸´æ—¶Pythonæ–‡ä»¶
        script_path = os.path.join(self.temp_dir, "script.py")
        with open(script_path, 'w') as f:
            f.write(code)
        
        try:
            # ä½¿ç”¨subprocessæ‰§è¡Œï¼Œé™åˆ¶èµ„æº
            result = subprocess.run(
                ["python", script_path],
                cwd=self.temp_dir,
                capture_output=True,
                text=True,
                timeout=self.timeout
            )
            
            output = result.stdout if result.returncode == 0 else result.stderr
            success = result.returncode == 0
            
            # æ›´æ–°æ–‡ä»¶ç³»ç»ŸçŠ¶æ€
            self._update_file_system()
            
            return output.strip(), success
            
        except subprocess.TimeoutExpired:
            return "Error: Code execution timeout", False
        except Exception as e:
            return f"Error: {str(e)}", False
    
    def _update_file_system(self):
        """æ›´æ–°æ–‡ä»¶ç³»ç»ŸçŠ¶æ€"""
        for filename in os.listdir(self.temp_dir):
            filepath = os.path.join(self.temp_dir, filename)
            if os.path.isfile(filepath) and filename != "script.py":
                with open(filepath, 'r') as f:
                    try:
                        self.state.file_system[filename] = f.read()
                    except:
                        pass  # å¿½ç•¥äºŒè¿›åˆ¶æ–‡ä»¶
    
    def _compute_final_reward(self, answer: str) -> float:
        """è®¡ç®—æœ€ç»ˆå¥–åŠ±ï¼ˆéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡å®ç°ï¼‰"""
        # è¿™æ˜¯ä¸€ä¸ªå ä½å®ç°ï¼Œå®é™…åº”ç”¨ä¸­éœ€è¦æ ¹æ®ä»»åŠ¡ç±»å‹å®šåˆ¶
        return 1.0  # ç®€åŒ–ï¼šå‡è®¾æäº¤å³æˆåŠŸ
    
    def _cleanup_temp_dir(self):
        """æ¸…ç†ä¸´æ—¶ç›®å½•"""
        if self.temp_dir and os.path.exists(self.temp_dir):
            import shutil
            shutil.rmtree(self.temp_dir)
    
    def __del__(self):
        self._cleanup_temp_dir()
```

### ç®—æ³•å®ç°

å®ç°åŸºäºPPOçš„LLM-in-Sandbox-RLè®­ç»ƒå™¨ï¼š

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer
from typing import List, Tuple
import numpy as np

class SandboxRLAgent:
    """
    LLM-in-Sandboxå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“
    åŸºäºPPOç®—æ³•è®­ç»ƒè¯­è¨€æ¨¡å‹åœ¨æ²™ç®±ä¸­çš„æ¢ç´¢èƒ½åŠ›
    """
    def __init__(
        self,
        model_name: str = "gpt2",  # ä½¿ç”¨å°æ¨¡å‹åšæ¼”ç¤º
        learning_rate: float = 1e-5,
        gamma: float = 0.99,  # æŠ˜æ‰£å› å­
        gae_lambda: float = 0.95,  # GAEå‚æ•°
        clip_epsilon: float = 0.2,  # PPOè£å‰ªå‚æ•°
        value_loss_coef: float = 0.5,
        entropy_coef: float = 0.01,
        device: str = "cuda" if torch.cuda.is_available() else "cpu"
    ):
        self.device = device
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.tokenizer.pad_token = self.tokenizer.eos_token
        
        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(device)
        
        # æ·»åŠ ä»·å€¼å¤´ï¼ˆValue Headï¼‰ç”¨äºä¼°è®¡çŠ¶æ€ä»·å€¼
        hidden_size = self.model.config.hidden_size
        self.value_head = nn.Linear(hidden_size, 1).to(device)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.Adam(
            list(self.model.parameters()) + list(self.value_head.parameters()),
            lr=learning_rate
        )
        
        # è¶…å‚æ•°
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_epsilon = clip_epsilon
        self.value_loss_coef = value_loss_coef
        self.entropy_coef = entropy_coef
        
    def select_action(
        self,
        state: SandboxState,
        deterministic: bool = False,
        max_new_tokens: int = 256
    ) -> Tuple[Dict[str, Any], torch.Tensor, torch.Tensor]:
        """
        æ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©åŠ¨ä½œ
        è¿”å›: (action, log_prob, value)
        """
        # æ„å»ºæç¤º
        prompt = self._build_prompt(state)
        
        # ç¼–ç è¾“å…¥
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=1024
        ).to(self.device)
        
        with torch.no_grad():
            # ç”ŸæˆåŠ¨ä½œï¼ˆæ–‡æœ¬ï¼‰
            if deterministic:
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=False,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            else:
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # è·å–æ¨¡å‹è¾“å‡ºç”¨äºè®¡ç®—ä»·å€¼
            model_outputs = self.model(**inputs, output_hidden_states=True)
            last_hidden_state = model_outputs.hidden_states[-1][:, -1, :]
            value = self.value_head(last_hidden_state)
        
        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        generated_text = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )
        
        # è§£æåŠ¨ä½œ
        action = self._parse_action(generated_text)
        
        # è®¡ç®—logæ¦‚ç‡ï¼ˆç”¨äºPPOæ›´æ–°ï¼‰
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥è®¡ç®—å®Œæ•´åºåˆ—çš„log_prob
        log_prob = self._compute_log_prob(inputs, outputs)
        
        return action, log_prob, value
    
    def _build_prompt(self, state: SandboxState) -> str:
        """æ„å»ºè¾“å…¥æç¤º"""
        prompt = "You are an AI assistant with access to a code sandbox. You can:\n"
        prompt += "1. Execute Python code by writing: CODE: <your code>\n"
        prompt += "2. Submit answer by writing: SUBMIT: <your answer>\n"
        prompt += "3. Provide text response by writing: TEXT: <your response>\n\n"
        prompt += f"Task: {state.task_description}\n\n"
        
        # æ·»åŠ å¯¹è¯å†å²
        for msg in state.conversation_history[-5:]:  # åªä¿ç•™æœ€è¿‘5è½®
            role = msg["role"]
            content = msg["content"]
            prompt += f"{role.upper()}: {content}\n"
        
        if state.last_output:
            prompt += f"\nLast execution output: {state.last_output}\n"
        
        prompt += "\nYour action: "
        return prompt
    
    def _parse_action(self, text: str) -> Dict[str, Any]:
        """è§£æç”Ÿæˆçš„æ–‡æœ¬ä¸ºåŠ¨ä½œ"""
        text = text.strip()
        
        if text.startswith("CODE:"):
            return {
                "type": "code",
                "content": text[5:].strip()
            }
        elif text.startswith("SUBMIT:"):
            return {
                "type": "submit",
                "answer": text[7:].strip()
            }
        else:
            # é»˜è®¤ä¸ºæ–‡æœ¬å›å¤
            if text.startswith("TEXT:"):
                text = text[5:].strip()
            return {
                "type": "text",
                "content": text
            }
    
    def _compute_log_prob(self, inputs, outputs):
        """è®¡ç®—åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡"""
        # ç®€åŒ–å®ç°ï¼šè¿”å›ä¸€ä¸ªå ä½å¼ é‡
        # å®é™…åº”è¯¥è®¡ç®—å®Œæ•´åºåˆ—çš„log_prob
        return torch.tensor(0.0, device=self.device)
    
    def compute_gae(
        self,
        rewards: List[float],
        values: List[torch.Tensor],
        dones: List[bool]
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        è®¡ç®—å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼ˆGeneralized Advantage Estimationï¼‰
        è¿”å›: (advantages, returns)
        """
        advantages = []
        returns = []
        gae = 0
        next_value = 0
        
        # ä»åå‘å‰è®¡ç®—
        for t in reversed(range(len(rewards))):
            if dones[t]:
                next_value = 0
                gae = 0
            
            # TDè¯¯å·®
            delta = rewards[t] + self.gamma * next_value - values[t].item()
            
            # GAEç´¯ç§¯
            gae = delta + self.gamma * self.gae_lambda * gae
            
            advantages.insert(0, gae)
            returns.insert(0, gae + values[t].item())
            
            next_value = values[t].item()
        
        advantages = torch.tensor(advantages, device=self.device)
        returns = torch.tensor(returns, device=self.device)
        
        # æ ‡å‡†åŒ–ä¼˜åŠ¿
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        return advantages, returns
    
    def update(
        self,
        states: List[SandboxState],
        actions: List[str],
        old_log_probs: torch.Tensor,
        advantages: torch.Tensor,
        returns: torch.Tensor,
        ppo_epochs: int = 4
    ):
        """
        PPOæ›´æ–°æ­¥éª¤
        """
        for _ in range(ppo_epochs):
            # é‡æ–°è®¡ç®—å½“å‰ç­–ç•¥ä¸‹çš„log_probså’Œvalues
            all_log_probs = []
            all_values = []
            all_entropies = []
            
            for state, action_text in zip(states, actions):
                # æ„å»ºè¾“å…¥
                prompt = self._build_prompt(state)
                inputs = self.tokenizer(
                    prompt + action_text,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=1024
                ).to(self.device)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(**inputs, output_hidden_states=True)
                logits = outputs.logits
                
                # è®¡ç®—log_probï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
                # å®é™…åº”è¯¥å¯¹æ•´ä¸ªç”Ÿæˆåºåˆ—è®¡ç®—
                log_probs = F.log_softmax(logits[:, -1, :], dim=-1)
                entropy = -(log_probs * log_probs.exp()).sum(dim=-1).mean()
                
                # è®¡ç®—ä»·å€¼
                last_hidden = outputs.hidden_states[-1][:, -1, :]
                value = self.value_head(last_hidden)
                
                all_log_probs.append(log_probs.max(dim=-1)[0])
                all_values.append(value)
                all_entropies.append(entropy)
            
            # è½¬æ¢ä¸ºå¼ é‡
            log_probs = torch.stack(all_log_probs)
            values = torch.cat(all_values)
            entropy = torch.stack(all_entropies).mean()
            
            # è®¡ç®—æ¯”ç‡
            ratio = torch.exp(log_probs - old_log_probs)
            
            # PPOè£å‰ªç›®æ ‡
            surr1 = ratio * advantages
            surr2 = torch.clamp(
                ratio,
                1.0 - self.clip_epsilon,
                1.0 + self.clip_epsilon
            ) * advantages
            
            # ç­–ç•¥æŸå¤±
            policy_loss = -torch.min(surr1, surr2).mean()
            
            # ä»·å€¼æŸå¤±
            value_loss = F.mse_loss(values.squeeze(), returns)
            
            # æ€»æŸå¤±
            loss = (
                policy_loss +
                self.value_loss_coef * value_loss -
                self.entropy_coef * entropy
            )
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(
                list(self.model.parameters()) + list(self.value_head.parameters()),
                max_norm=0.5
            )
            self.optimizer.step()
            
        return {
            "policy_loss": policy_loss.item(),
            "value_loss": value_loss.item(),
            "entropy": entropy.item()
        }
```

### è®­ç»ƒå¾ªç¯

```python
import matplotlib.pyplot as plt
from tqdm import tqdm

class SandboxTrainer:
    """LLM-in-Sandboxè®­ç»ƒå™¨"""
    
    def __init__(
        self,
        agent: SandboxRLAgent,
        env: SimpleSandboxEnv,
        num_iterations: int = 100,
        episodes_per_iteration: int = 4
    ):
        self.agent = agent
        self.env = env
        self.num_iterations = num_iterations
        self.episodes_per_iteration = episodes_per_iteration
        
        # è®°å½•è®­ç»ƒæŒ‡æ ‡
        self.metrics = {
            "episode_rewards": [],
            "episode_lengths": [],
            "policy_losses": [],
            "value_losses": [],
            "success_rate": []
        }
    
    def train(self, tasks: List[str]):
        """
        ä¸»è®­ç»ƒå¾ªç¯
        tasks: è®­ç»ƒä»»åŠ¡åˆ—è¡¨
        """
        print("å¼€å§‹è®­ç»ƒ LLM-in-Sandbox-RL...")
        
        for iteration in tqdm(range(self.num_iterations)):
            # æ”¶é›†è½¨è¿¹
            trajectories = []
            iteration_rewards = []
            iteration_lengths = []
            successes = []
            
            for _ in range(self.episodes_per_iteration):
                # éšæœºé€‰æ‹©ä»»åŠ¡
                task = np.random.choice(tasks)
                trajectory = self._collect_trajectory(task)
                trajectories.append(trajectory)
                
                # è®°å½•æŒ‡æ ‡
                episode_reward = sum([t[2] for t in trajectory])
                iteration_rewards.append(episode_reward)
                iteration_lengths.append(len(trajectory))
                successes.append(trajectory[-1][3].get("success", False))
            
            # è®¡ç®—ä¼˜åŠ¿å’Œå›æŠ¥
            all_advantages = []
            all_returns = []
            
            for trajectory in trajectories:
                states, actions, rewards, infos, log_probs, values = zip(*trajectory)
                dones = [info.get("done", False) for info in infos]
                
                advantages, returns = self.agent.compute_gae(
                    list(rewards),
                    list(values),
                    dones
                )
                all_advantages.append(advantages)
                all_returns.append(returns)
            
            # åˆå¹¶æ‰€æœ‰è½¨è¿¹æ•°æ®
            all_states = []
            all_actions = []
            all_old_log_probs = []
            
            for i, trajectory in enumerate(trajectories):
                states, actions, _, _, log_probs, _ = zip(*trajectory)
                all_states.extend(states)
                all_actions.extend([self._action_to_text(a) for a in actions])
                all_old_log_probs.extend(log_probs)
            
            advantages = torch.cat(all_advantages)
            returns = torch.cat(all_returns)
            old_log_probs = torch.stack(all_old_log_probs)
            
            # PPOæ›´æ–°
            update_info = self.agent.update(
                all_states,
                all_actions,
                old_log_probs,
                advantages,
                returns
            )
            
            # è®°å½•æŒ‡æ ‡
            self.metrics["episode_rewards"].append(np.mean(iteration_rewards))
            self.metrics["episode_lengths"].append(np.mean(iteration_lengths))
            self.metrics["policy_losses"].append(update_info["policy_loss"])
            self.metrics["value_losses"].append(update_info["value_loss"])
            self.metrics["success_rate"].append(np.mean(successes))
            
            # å®šæœŸæ‰“å°
            if (iteration + 1) % 10 == 0:
                print(f"\nè¿­ä»£ {iteration + 1}/{self.num_iterations}")
                print(f"  å¹³å‡å¥–åŠ±: {np.mean(iteration_rewards):.2f}")
                print(f"  æˆåŠŸç‡: {np.mean(successes):.2%}")
                print(f"  ç­–ç•¥æŸå¤±: {update_info['policy_loss']:.4f}")
        
        print("\nè®­ç»ƒå®Œæˆï¼")
        return self.metrics
    
    def _collect_trajectory(self, task: str) -> List[Tuple]:
        """
        æ”¶é›†ä¸€æ¡å®Œæ•´è½¨è¿¹
        è¿”å›: [(state, action, reward, info, log_prob, value), ...]
        """
        trajectory = []
        state = self.env.reset(task)
        done = False
        
        while not done:
            # é€‰æ‹©åŠ¨ä½œ
            action, log_prob, value = self.agent.select_action(state)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, info = self.env.step(action)
            
            # ä¿å­˜è½¬ç§»
            trajectory.append((
                state,
                action,
                reward,
                info,
                log_prob,
                value
            ))
            
            state = next_state
            
            # é˜²æ­¢æ— é™å¾ªç¯
            if len(trajectory) >= self.env.max_steps:
                break
        
        return trajectory
    
    def _action_to_text(self, action: Dict[str, Any]) -> str:
        """å°†åŠ¨ä½œå­—å…¸è½¬æ¢ä¸ºæ–‡æœ¬"""
        if action["type"] == "code":
            return f"CODE: {action['content']}"
        elif action["type"] == "submit":
            return f"SUBMIT: {action.get('answer', '')}"
        else:
            return f"TEXT: {action['content']}"
    
    def plot_metrics(self):
        """å¯è§†åŒ–è®­ç»ƒæŒ‡æ ‡"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # å¥–åŠ±æ›²çº¿
        axes[0, 0].plot(self.metrics["episode_rewards"])
        axes[0, 0].set_title("Episode Rewards")
        axes[0, 0].set_xlabel("Iteration")
        axes[0, 0].set_ylabel("Reward")
        axes[0, 0].grid(True)
        
        # æˆåŠŸç‡
        axes[0, 1].plot(self.metrics["success_rate"])
        axes[0, 1].set_title("Success Rate")
        axes[0, 1].set_xlabel("Iteration")
        axes[0, 1].set_ylabel("Success Rate")
        axes[0, 1].grid(True)
        
        # ç­–ç•¥æŸå¤±
        axes[1, 0].plot(self.metrics["policy_losses"])
        axes[1, 0].set_title("Policy Loss")
        axes[1, 0].set_xlabel("Iteration")
        axes[1, 0].set_ylabel("Loss")
        axes[1, 0].grid(True)
        
        # ä»·å€¼æŸå¤±
        axes[1, 1].plot(self.metrics["value_losses"])
        axes[1, 1].set_title("Value Loss")
        axes[1, 1].set_xlabel("Iteration")
        axes[1, 1].set_ylabel("Loss")
        axes[1, 1].grid(True)
        
        plt.tight_layout()
        plt.savefig("training_metrics.png", dpi=300)
        print("è®­ç»ƒæ›²çº¿å·²ä¿å­˜è‡³ training_metrics.png")
```

### å®Œæ•´è®­ç»ƒç¤ºä¾‹

```python
# å®šä¹‰ç®€å•çš„æ•°å­¦ä»»åŠ¡é›†
math_tasks = [
    "è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬10é¡¹",
    "æ±‚è§£æ–¹ç¨‹ x^2 - 5x + 6 = 0",
    "è®¡ç®—1åˆ°100çš„æ‰€æœ‰è´¨æ•°ä¹‹å’Œ",
    "ç”Ÿæˆä¸€ä¸ª10x10çš„ä¹˜æ³•è¡¨å¹¶ä¿å­˜åˆ°æ–‡ä»¶",
    "è®¡ç®—åœ†å‘¨ç‡Ï€çš„å‰100ä½å°æ•°"
]

# åˆå§‹åŒ–ç¯å¢ƒå’Œæ™ºèƒ½ä½“
env = SimpleSandboxEnv(max_steps=10, timeout=5)
agent = SandboxRLAgent(
    model_name="gpt2",  # å¯æ›¿æ¢ä¸ºæ›´å¤§çš„æ¨¡å‹
    learning_rate=1e-5,
    gamma=0.99,
    clip_epsilon=0.2
)

# åˆ›å»ºè®­ç»ƒå™¨
trainer = SandboxTrainer(
    agent=agent,
    env=env,
    num_iterations=100,
    episodes_per_iteration=4
)

# å¼€å§‹è®­ç»ƒ
metrics = trainer.train(math_tasks)

# å¯è§†åŒ–ç»“æœ
trainer.plot_metrics()

# æµ‹è¯•è®­ç»ƒåçš„æ™ºèƒ½ä½“
print("\n=== æµ‹è¯•è®­ç»ƒåçš„æ™ºèƒ½ä½“ ===")
test_task = "è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬15é¡¹"
state = env.reset(test_task)
done = False
step = 0

print(f"ä»»åŠ¡: {test_task}\n")
while not done and step < 5:
    action, _, _ = agent.select_action(state, deterministic=True)
    print(f"æ­¥éª¤ {step + 1}:")
    print(f"  åŠ¨ä½œç±»å‹: {action['type']}")
    print(f"  å†…å®¹: {action.get('content', action.get('answer', ''))[:100]}")
    
    state, reward, done, info = env.step(action)
    print(f"  å¥–åŠ±: {reward}")
    print(f"  å®Œæˆ: {done}\n")
    
    step += 1
```

## é«˜çº§æŠ€å·§

### æŠ€å·§1ï¼šå¤šæ¨¡æ€å¥–åŠ±å¡‘å½¢

å•çº¯çš„ä»»åŠ¡å®Œæˆå¥–åŠ±æ˜¯ç¨€ç–çš„ï¼Œæˆ‘ä»¬å¯ä»¥è®¾è®¡æ›´ç»†ç²’åº¦çš„å¥–åŠ±å‡½æ•°ï¼š

```python
class AdvancedRewardShaper:
    """é«˜çº§å¥–åŠ±å¡‘å½¢å™¨"""
    
    def __init__(self):
        self.reward_weights = {
            "code_execution_success": 0.1,
            "file_operation": 0.05,
            "progress_toward_goal": 0.2,
            "efficiency": 0.15,
            "final_correctness": 1.0
        }
    
    def compute_shaped_reward(
        self,
        action: Dict[str, Any],
        state: SandboxState,
        next_state: SandboxState,
        base_reward: float
    ) -> float:
        """
        è®¡ç®—å¡‘å½¢åçš„å¥–åŠ±
        """
        shaped_reward = base_reward
        
        # 1. ä»£ç æ‰§è¡ŒæˆåŠŸå¥–åŠ±
        if action["type"] == "code" and next_state.last_output:
            if "Error" not in next_state.last_output:
                shaped_reward += self.reward_weights["code_execution_success"]
        
        # 2. æ–‡ä»¶æ“ä½œå¥–åŠ±ï¼ˆé¼“åŠ±ä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿï¼‰
        if len(next_state.file_system) > len(state.file_system):
            shaped_reward += self.reward_weights["file_operation"]
        
        # 3. è¿›åº¦å¥–åŠ±ï¼ˆåŸºäºä»»åŠ¡å…³é”®è¯åŒ¹é…ï¼‰
        progress_score = self._estimate_progress(state, next_state)
        shaped_reward += progress_score * self.reward_weights["progress_toward_goal"]
        
        # 4. æ•ˆç‡æƒ©ç½šï¼ˆè¿‡å¤šæ­¥éª¤ï¼‰
        if len(state.conversation_history) > 10:
            shaped_reward -= self.reward_weights["efficiency"] * 0.1
        
        return shaped_reward
    
    def _estimate_progress(
        self,
        state: SandboxState,
        next_state: SandboxState
    ) -> float:
        """
        ä¼°è®¡ä»»åŠ¡è¿›åº¦ï¼ˆç®€åŒ–å®ç°ï¼‰
        å®é™…å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„å¯å‘å¼æˆ–å­¦ä¹ çš„è¿›åº¦ä¼°è®¡å™¨
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„æœ‰æ„ä¹‰è¾“å‡º
        if next_state.last_output and next_state.last_output != state.last_output:
            # æ£€æŸ¥è¾“å‡ºæ˜¯å¦åŒ…å«æ•°å­—ï¼ˆå¯¹æ•°å­¦ä»»åŠ¡æœ‰ç”¨ï¼‰
            import re
            if re.search(r'\d+', next_state.last_output):
                return 0.5
        return 0.0

# é›†æˆåˆ°ç¯å¢ƒä¸­
class EnhancedSandboxEnv(SimpleSandboxEnv):
    """å¢å¼ºçš„æ²™ç®±ç¯å¢ƒï¼Œæ”¯æŒå¥–åŠ±å¡‘å½¢"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.reward_shaper = AdvancedRewardShaper()
    
    def step(self, action: Dict[str, Any]) -> Tuple[SandboxState, float, bool, Dict]:
        old_state = self._copy_state(self.state)
        next_state, base_reward, done, info = super().step(action)
        
        # åº”ç”¨å¥–åŠ±å¡‘å½¢
        shaped_reward = self.reward_shaper.compute_shaped_reward(
            action, old_state, next_state, base_reward
        )
        
        return next_state, shaped_reward, done, info
    
    def _copy_state(self, state: SandboxState) -> SandboxState:
        """æ·±æ‹·è´çŠ¶æ€"""
        from copy import deepcopy
        return deepcopy(state)
```

**æ€§èƒ½æå‡åˆ†æ**ï¼š
- å¥–åŠ±å¡‘å½¢å¯ä»¥å°†å¹³å‡æ”¶æ•›é€Ÿåº¦æå‡30-50%
- å‡å°‘äº†æ¢ç´¢é˜¶æ®µçš„éšæœºæ€§
- ç‰¹åˆ«é€‚ç”¨äºé•¿æœŸä»»åŠ¡ï¼ˆ>5æ­¥ï¼‰

### æŠ€å·§2ï¼šè¯¾ç¨‹å­¦ä¹ ï¼ˆCurriculum Learningï¼‰

ä»ç®€å•ä»»åŠ¡é€æ­¥è¿‡æ¸¡åˆ°å¤æ‚ä»»åŠ¡ï¼š

```python
class CurriculumManager:
    """è¯¾ç¨‹å­¦ä¹ ç®¡ç†å™¨"""
    
    def __init__(self, task_pool: Dict[str, List[str]]):
        """
        task_pool: {"easy": [...], "medium": [...], "hard": [...]}
        """
        self.task_pool = task_pool
        self.current_level = "easy"
        self.success_threshold = 0.7  # æˆåŠŸç‡é˜ˆå€¼
        self.recent_success_rate = []
        self.window_size = 20  # æ»‘åŠ¨çª—å£å¤§å°
    
    def get_task(self) -> str:
        """æ ¹æ®å½“å‰éš¾åº¦çº§åˆ«è·å–ä»»åŠ¡"""
        return np.random.choice(self.task_pool[self.current_level])
    
    def update(self, success: bool):
        """æ›´æ–°æˆåŠŸç‡å¹¶å¯èƒ½æå‡éš¾åº¦"""
        self.recent_success_rate.append(float(success))
        
        # ä¿æŒå›ºå®šçª—å£å¤§å°
        if len(self.recent_success_rate) > self.window_size:
            self.recent_success_rate.pop(0)
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥æå‡éš¾åº¦
        if len(self.recent_success_rate) >= self.window_size:
            avg_success = np.mean(self.recent_success_rate)
            
            if avg_success >= self.success_threshold:
                if self.current_level == "easy":
                    self.current_level = "medium"
                    self.recent_success_rate = []
                    print("ğŸ“ˆ éš¾åº¦æå‡è‡³: medium")
                elif self.current_level == "medium":
                    self.current_level = "hard"
                    self.recent_success_rate = []
                    print("ğŸ“ˆ éš¾åº¦æå‡è‡³: hard")
    
    def get_difficulty(self) -> str:
        """è·å–å½“å‰éš¾åº¦"""
        return self.current_level

# ä½¿ç”¨è¯¾ç¨‹å­¦ä¹ çš„è®­ç»ƒå™¨
class CurriculumTrainer(SandboxTrainer):
    """æ”¯æŒè¯¾ç¨‹å­¦ä¹ çš„è®­ç»ƒå™¨"""
    
    def __init__(self, agent, env, curriculum_manager, *args, **kwargs):
        super().__init__(agent, env, *args, **kwargs)
        self.curriculum = curriculum_manager
        self.metrics["difficulty_levels"] = []
    
    def train(self):
        """ä½¿ç”¨è¯¾ç¨‹å­¦ä¹ çš„è®­ç»ƒå¾ªç¯"""
        print("å¼€å§‹è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ...")
        
        for iteration in tqdm(range(self.num_iterations)):
            trajectories = []
            iteration_rewards = []
            successes = []
            
            for _ in range(self.episodes_per_iteration):
                # ä»è¯¾ç¨‹ç®¡ç†å™¨è·å–ä»»åŠ¡
                task = self.curriculum.get_task()
                trajectory = self._collect_trajectory(task)
                trajectories.append(trajectory)
                
                # è®°å½•æˆåŠŸæƒ…å†µ
                success = trajectory[-1][3].get("success", False)
                successes.append(success)
                
                # æ›´æ–°è¯¾ç¨‹
                self.curriculum.update(success)
                
                episode_reward = sum([t[2] for t in trajectory])
                iteration_rewards.append(episode_reward)
            
            # è®°å½•å½“å‰éš¾åº¦
            self.metrics["difficulty_levels"].append(self.curriculum.get_difficulty())
            
            # ... å…¶ä½™è®­ç»ƒé€»è¾‘åŒåŸºç¡€ç‰ˆæœ¬
            
        return self.metrics

# ä½¿ç”¨ç¤ºä¾‹
task_curriculum = {
    "easy": [
        "è®¡ç®— 2 + 2",
        "æ‰“å° 'Hello World'",
        "åˆ›å»ºä¸€ä¸ªåŒ…å«æ•°å­—1-5çš„åˆ—è¡¨"
    ],
    "medium": [
        "è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬10é¡¹",
        "æ±‚è§£æ–¹ç¨‹ x^2 - 5x + 6 = 0",
        "ç”Ÿæˆ1åˆ°50çš„è´¨æ•°åˆ—è¡¨"
    ],
    "hard": [
        "å®ç°å¿«é€Ÿæ’åºç®—æ³•å¹¶æ’åºéšæœºæ•°ç»„",
        "è®¡ç®—åœ†å‘¨ç‡Ï€çš„å‰100ä½å°æ•°",
        "æ±‚è§£å¾®åˆ†æ–¹ç¨‹ dy/dx = x^2 çš„æ•°å€¼è§£"
    ]
}

curriculum = CurriculumManager(task_curriculum)
curriculum_trainer = CurriculumTrainer(
    agent=agent,
    env=env,
    curriculum_manager=curriculum,
    num_iterations=150,
    episodes_per_iteration=4
)

metrics = curriculum_trainer.train()
```

**æ€§èƒ½æå‡åˆ†æ**ï¼š
- è¯¾ç¨‹å­¦ä¹ å¯ä»¥å‡å°‘40-60%çš„è®­ç»ƒæ—¶é—´
- æé«˜æœ€ç»ˆæ€§èƒ½ä¸Šé™ï¼ˆåœ¨å›°éš¾ä»»åŠ¡ä¸Šæå‡15-25%ï¼‰
- è®­ç»ƒè¿‡ç¨‹æ›´ç¨³å®šï¼Œæ–¹å·®æ›´å°

### æŠ€å·§3ï¼šç»éªŒå›æ”¾ä¸ä¼˜å…ˆçº§é‡‡æ ·

```python
import random
from collections import deque, namedtuple

Transition = namedtuple('Transition', 
                       ['state', 'action', 'reward', 'next_state', 'done', 'priority'])

class PrioritizedReplayBuffer:
    """ä¼˜å…ˆçº§ç»éªŒå›æ”¾ç¼“å†²åŒº"""
    
    def __init__(self, capacity: int = 10000, alpha: float = 0.6):
        self.buffer = deque(maxlen=capacity)
        self.alpha = alpha  # ä¼˜å…ˆçº§æŒ‡æ•°
        self.priorities = deque(maxlen=capacity)
        self.max_priority = 1.0
    
    def push(self, state, action, reward, next_state, done):
        """æ·»åŠ ç»éªŒ"""
        # æ–°ç»éªŒä½¿ç”¨æœ€å¤§ä¼˜å…ˆçº§
        transition = Transition(state, action, reward, next_state, done, self.max_priority)
        self.buffer.append(transition)
        self.priorities.append(self.max_priority)
    
    def sample(self, batch_size: int, beta: float = 0.4):
        """
        ä¼˜å…ˆçº§é‡‡æ ·
        beta: é‡è¦æ€§é‡‡æ ·æƒé‡æŒ‡æ•°
        """
        if len(self.buffer) < batch_size:
            return None
        
        # è®¡ç®—é‡‡æ ·æ¦‚ç‡
        priorities = np.array(self.priorities)
        probs = priorities ** self.alpha
        probs /= probs.sum()
        
        # é‡‡æ ·ç´¢å¼•
        indices = np.random.choice(len(self.buffer), batch_size, p=probs, replace=False)
        
        # è®¡ç®—é‡è¦æ€§é‡‡æ ·æƒé‡
        weights = (len(self.buffer) * probs[indices]) ** (-beta)
        weights /= weights.max()
        
        # è·å–æ ·æœ¬
        samples = [self.buffer[idx] for idx in indices]
        
        return samples, weights, indices
    
    def update_priorities(self, indices: List[int], td_errors: np.ndarray):
        """æ ¹æ®TDè¯¯å·®æ›´æ–°ä¼˜å…ˆçº§"""
        for idx, td_error in zip(indices, td_errors):
            priority = abs(td_error) + 1e-6  # é¿å…é›¶ä¼˜å…ˆçº§
            self.priorities[idx] = priority
            self.max_priority = max(self.max_priority, priority)
    
    def __len__(self):
        return len(self.buffer)

# é›†æˆåˆ°è®­ç»ƒå™¨
class ReplayBasedTrainer(SandboxTrainer):
    """åŸºäºç»éªŒå›æ”¾çš„è®­ç»ƒå™¨"""
    
    def __init__(self, agent, env, buffer_size=10000, *args, **kwargs):
        super().__init__(agent, env, *args, **kwargs)
        self.replay_buffer = PrioritizedReplayBuffer(capacity=buffer_size)
        self.batch_size = 32
    
    def train(self, tasks: List[str]):
        """ä½¿ç”¨ç»éªŒå›æ”¾çš„è®­ç»ƒ"""
        print("å¼€å§‹åŸºäºç»éªŒå›æ”¾çš„è®­ç»ƒ...")
        
        # é¦–å…ˆæ”¶é›†ä¸€äº›åˆå§‹ç»éªŒ
        print("æ”¶é›†åˆå§‹ç»éªŒ...")
        for _ in tqdm(range(50)):
            task = np.random.choice(tasks)
            trajectory = self._collect_trajectory(task)
            
            # æ·»åŠ åˆ°å›æ”¾ç¼“å†²åŒº
            for state, action, reward, info, _, _ in trajectory:
                next_state = state  # ç®€åŒ–å¤„ç†
                done = info.get("done", False)
                self.replay_buffer.push(state, action, reward, next_state, done)
        
        # ä¸»è®­ç»ƒå¾ªç¯
        for iteration in tqdm(range(self.num_iterations)):
            # 1. æ”¶é›†æ–°ç»éªŒ
            for _ in range(self.episodes_per_iteration):
                task = np.random.choice(tasks)
                trajectory = self._collect_trajectory(task)
                
                for state, action, reward, info, _, _ in trajectory:
                    next_state = state
                    done = info.get("done", False)
                    self.replay_buffer.push(state, action, reward, next_state, done)
            
            # 2. ä»å›æ”¾ç¼“å†²åŒºé‡‡æ ·å¹¶æ›´æ–°
            if len(self.replay_buffer) >= self.batch_size:
                samples, weights, indices = self.replay_buffer.sample(
                    self.batch_size,
                    beta=0.4 + iteration / self.num_iterations * 0.6  # çº¿æ€§é€€ç«
                )
                
                # æ‰§è¡Œæ›´æ–°ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
                # å®é™…åº”è¯¥è®¡ç®—TDè¯¯å·®å¹¶æ›´æ–°ä¼˜å…ˆçº§
                # ...
                
        return self.metrics
```

## å®éªŒåˆ†æ

### åŸºå‡†æµ‹è¯•è®¾ç½®

æˆ‘ä»¬åœ¨ä»¥ä¸‹ä»»åŠ¡ç±»å‹ä¸Šè¯„ä¼°LLM-in-Sandbox-RLï¼š

```python
# è¯„ä¼°ä»»åŠ¡é›†
evaluation_tasks = {
    "æ•°å­¦è®¡ç®—": [
        "è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬20é¡¹",
        "æ±‚è§£äºŒæ¬¡æ–¹ç¨‹ 2x^2 - 7x + 3 = 0",
        "è®¡ç®—1åˆ°1000ä¸­æ‰€æœ‰èƒ½è¢«7æ•´é™¤çš„æ•°çš„å’Œ"
    ],
    "æ•°æ®å¤„ç†": [
        "è¯»å–CSVæ–‡ä»¶å¹¶è®¡ç®—å¹³å‡å€¼",
        "ç”Ÿæˆ100ä¸ªéšæœºæ•°å¹¶ç»˜åˆ¶ç›´æ–¹å›¾",
        "å°†JSONæ•°æ®è½¬æ¢ä¸ºæ ¼å¼åŒ–è¡¨æ ¼"
    ],
    "ç®—æ³•å®ç°": [
        "å®ç°äºŒåˆ†æŸ¥æ‰¾ç®—æ³•",
        "å®ç°å†’æ³¡æ’åºå¹¶æµ‹è¯•æ€§èƒ½",
        "å®ç°æ·±åº¦ä¼˜å…ˆæœç´¢éå†æ ‘ç»“æ„"
    ]
}

def evaluate_agent(agent, env, tasks, num_trials=10):
    """è¯„ä¼°æ™ºèƒ½ä½“æ€§èƒ½"""
    results = {
        "success_rate": [],
        "average_steps": [],
        "average_reward": []
    }
    
    for task_category, task_list in tasks.items():
        print(f"\nè¯„ä¼°ç±»åˆ«: {task_category}")
        category_successes = []
        category_steps = []
        category_rewards = []
        
        for task in task_list:
            trial_successes = []
            trial_steps = []
            trial_rewards = []
            
            for _ in range(num_trials):
                state = env.reset(task)
                done = False
                steps = 0
                total_reward = 0
                
                while not done and steps < env.max_steps:
                    action, _, _ = agent.select_action(state, deterministic=True)
                    state, reward, done, info = env.step(action)
                    total_reward += reward
                    steps += 1
                
                trial_successes.append(info.get("success", False))
                trial_steps.append(steps)
                trial_rewards.append(total_reward)
            
            category_successes.extend(trial_successes)
            category_steps.extend(trial_steps)
            category_rewards.extend(trial_rewards)
            
            print(f"  {task[:50]}...")
            print(f"    æˆåŠŸç‡: {np.mean(trial_successes):.2%}")
            print(f"    å¹³å‡æ­¥æ•°: {np.mean(trial_steps):.1f}")
        
        results["success_rate"].append(np.mean(category_successes))
        results["average_steps"].append(np.mean(category_steps))
        results["average_reward"].append(np.mean(category_rewards))
    
    return results

# è¿è¡Œè¯„ä¼°
print("=== è¯„ä¼°è®­ç»ƒåçš„æ™ºèƒ½ä½“ ===")
eval_results = evaluate_agent(agent, env, evaluation_tasks, num_trials=5)
```

### è¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æ

```python
def hyperparameter_sensitivity_study():
    """è¶…å‚æ•°æ•æ„Ÿæ€§ç ”ç©¶"""
    
    # æµ‹è¯•ä¸åŒçš„å­¦ä¹ ç‡
    learning_rates = [1e-6, 5e-6, 1e-5, 5e-5, 1e-4]
    lr_results = []
    
    print("æµ‹è¯•å­¦ä¹ ç‡...")
    for lr in learning_rates:
        agent = SandboxRLAgent(learning_rate=lr)
        trainer = SandboxTrainer(agent, env, num_iterations=20)
        metrics = trainer.train(math_tasks[:3])  # ä½¿ç”¨ç®€åŒ–ä»»åŠ¡é›†
        final_success_rate = np.mean(metrics["success_rate"][-5:])
        lr_results.append(final_success_rate)
        print(f"  LR={lr}: æœ€ç»ˆæˆåŠŸç‡={final_success_rate:.2%}")
    
    # æµ‹è¯•ä¸åŒçš„clip_epsilon
    clip_epsilons = [0.1, 0.2, 0.3, 0.4]
    epsilon_results = []
    
    print("\næµ‹è¯•PPOè£å‰ªå‚æ•°...")
    for eps in clip_epsilons:
        agent = SandboxRLAgent(clip_epsilon=eps)
        trainer = SandboxTrainer(agent, env, num_iterations=20)
        metrics = trainer.train(math_tasks[:3])
        final_success_rate = np.mean(metrics["success_rate"][-5:])
        epsilon_results.append(final_success_rate)
        print(f"  Îµ={eps}: æœ€ç»ˆæˆåŠŸç‡={final_success_rate:.2%}")
    
    # å¯è§†åŒ–ç»“æœ
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
    
    ax1.plot(learning_rates, lr_results, marker='o')
    ax1.set_xscale('log')
    ax1.set_xlabel('Learning Rate')
    ax1.set_ylabel('Final Success Rate')
    ax1.set_title('Learning Rate Sensitivity')
    ax1.grid(True)
    
    ax2.plot(clip_epsilons, epsilon_results, marker='o')
    ax2.set_xlabel('Clip Epsilon')
    ax2.set_ylabel('Final Success Rate')
    ax2.set_title('PPO Clip Parameter Sensitivity')
    ax2.grid(True)
    
    plt.tight_layout()
    plt.savefig('hyperparameter_sensitivity.png', dpi=300)
    print("\nè¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æå›¾å·²ä¿å­˜")
```

### ä¸Baselineå¯¹æ¯”

```python
def compare_with_baselines():
    """ä¸åŸºçº¿æ–¹æ³•å¯¹æ¯”"""
    
    baselines = {
        "Random": lambda: random_agent_baseline(),
        "Supervised-Only": lambda: supervised_only_baseline(),
        "LLM-in-Sandbox-RL": lambda: trained_agent
    }
    
    results = {}
    
    for name, agent_fn in baselines.items():
        print(f"\nè¯„ä¼°: {name}")
        agent = agent_fn()
        eval_results = evaluate_agent(agent, env, evaluation_tasks, num_trials=5)
        results[name] = eval_results
    
    # å¯è§†åŒ–å¯¹æ¯”
    categories = list(evaluation_tasks.keys())
    x = np.arange(len(categories))
    width = 0.25
    
    fig, ax = plt.subplots(figsize=(10, 6))
    
    for i, (name, result) in enumerate(results.items()):
        ax.bar(x + i * width, result["success_rate"], width, label=name)
    
    ax.set_xlabel('Task Category')
    ax.set_ylabel('Success Rate')
    ax.set_title('Performance Comparison Across Task Categories')
    ax.set_xticks(x + width)
    ax.set_xticklabels(categories)
    ax.legend()
    ax.grid(True, axis='y')
    
    plt.tight_layout()
    plt.savefig('baseline_comparison.png', dpi=300)
    print("\nåŸºçº¿å¯¹æ¯”å›¾å·²ä¿å­˜")

def random_agent_baseline():
    """éšæœºåŠ¨ä½œåŸºçº¿"""
    class RandomAgent:
        def select_action(self, state, deterministic=False):
            action_type = np.random.choice(["code", "text", "submit"])
            if action_type == "code":
                return {"type": "code", "content": "print('random')"}, None, None
            elif action_type == "submit":
                return {"type": "submit", "answer": "42"}, None, None
            else:
                return {"type": "text", "content": "I don't know"}, None, None
    return RandomAgent()

def supervised_only_baseline():
    """ä»…ç›‘ç£å­¦ä¹ åŸºçº¿ï¼ˆæœªç»RLè®­ç»ƒï¼‰"""
    return SandboxRLAgent()  # æœªè®­ç»ƒçš„æ¨¡å‹
```

## å®é™…åº”ç”¨æ¡ˆä¾‹

### å¤æ‚ä»»åŠ¡ï¼šå¤šæ­¥éª¤æ•°æ®åˆ†æ

```python
class DataAnalysisTask:
    """
    å®é™…åº”ç”¨æ¡ˆä¾‹ï¼šå¤šæ­¥éª¤æ•°æ®åˆ†æä»»åŠ¡
    ä»»åŠ¡ï¼šåˆ†æé”€å”®æ•°æ®ï¼Œç”ŸæˆæŠ¥å‘Š
    """
    
    @staticmethod
    def create_sample_data():
        """åˆ›å»ºç¤ºä¾‹æ•°æ®"""
        import pandas as pd
        
        data = {
            'date': pd.date_range('2024-01-01', periods=100),
            'product': np.random.choice(['A', 'B', 'C'], 100),
            'sales': np.random.randint(100, 1000, 100),
            'region': np.random.choice(['North', 'South', 'East', 'West'], 100)
        }
        
        df = pd.DataFrame(data)
        df.to_csv('/tmp/sales_data.csv', index=False)
        return df
    
    @staticmethod
    def get_task_description():
        """è·å–ä»»åŠ¡æè¿°"""
        return """
        åˆ†æé”€å”®æ•°æ®æ–‡ä»¶ /tmp/sales_data.csvï¼Œå®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
        1. è®¡ç®—æ¯ä¸ªäº§å“çš„æ€»é”€å”®é¢
        2. æ‰¾å‡ºé”€å”®é¢æœ€é«˜çš„åœ°åŒº
        3. ç»˜åˆ¶é”€å”®è¶‹åŠ¿å›¾å¹¶ä¿å­˜
        4. ç”ŸæˆåŒ…å«ä»¥ä¸Šä¿¡æ¯çš„æ–‡æœ¬æŠ¥å‘Š
        """
    
    @staticmethod
    def verify_solution(file_system: Dict[str, str]) -> float:
        """éªŒè¯è§£å†³æ–¹æ¡ˆçš„è´¨é‡"""
        score = 0.0
        
        # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†æŠ¥å‘Šæ–‡ä»¶
        if 'report.txt' in file_system:
            score += 0.3
            report = file_system['report.txt']
            
            # æ£€æŸ¥æŠ¥å‘Šå†…å®¹
            if 'product' in report.lower():
                score += 0.2
            if 'region' in report.lower():
                score += 0.2
        
        # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†å›¾è¡¨
        if any('plot' in f or 'chart' in f for f in file_system.keys()):
            score += 0.3
        
        return score

# è¿è¡Œå¤æ‚ä»»åŠ¡
print("=== å¤æ‚ä»»åŠ¡æ¼”ç¤ºï¼šæ•°æ®åˆ†æ ===")

# å‡†å¤‡æ•°æ®
data_task = DataAnalysisTask()
sample_data = data_task.create_sample_data()
task_desc = data_task.get_task_description()

# ä½¿ç”¨è®­ç»ƒåçš„æ™ºèƒ½ä½“
env = EnhancedSandboxEnv(max_steps=15, timeout=10)
state = env.reset(task_desc)
done = False
step = 0

print(f"ä»»åŠ¡æè¿°:\n{task_desc}\n")
print("æ™ºèƒ½ä½“æ‰§è¡Œè¿‡ç¨‹:\n")

while not done and step < 15:
    action, _, _ = agent.select_action(state, deterministic=True)
    
    print(f"æ­¥éª¤ {step + 1}:")
    print(f"  åŠ¨ä½œ: {action['type']}")
    
    if action['type'] == 'code':
        print(f"  ä»£ç :\n{action['content'][:200]}...")
    elif action['type'] == 'text':
        print(f"  å›å¤: {action['content'][:100]}...")
    
    state, reward, done, info = env.step(action)
    
    if state.last_output:
        print(f"  è¾“å‡º: {state.last_output[:100]}...")
    print(f"  å¥–åŠ±: {reward:.2f}\n")
    
    step += 1

# è¯„ä¼°ç»“æœ
final_score = data_task.verify_solution(state.file_system)
print(f"\næœ€ç»ˆå¾—åˆ†: {final_score:.2%}")
print(f"ç”Ÿæˆçš„æ–‡ä»¶: {list(state.file_system.keys())}")
```

## è°ƒè¯•æŠ€å·§

### å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

```python
class SandboxDebugger:
    """LLM-in-Sandboxè°ƒè¯•å·¥å…·"""
    
    @staticmethod
    def diagnose_training_issues(metrics: Dict):
        """è¯Šæ–­è®­ç»ƒé—®é¢˜"""
        print("=== è®­ç»ƒè¯Šæ–­ ===\n")
        
        # 1. æ£€æŸ¥å¥–åŠ±ä¿¡å·
        rewards = metrics["episode_rewards"]
        if len(rewards) > 10:
            recent_trend = np.polyfit(range(len(rewards[-20:])), rewards[-20:], 1)[0]
            
            if abs(recent_trend) < 0.01:
                print("âš ï¸  è­¦å‘Š: å¥–åŠ±åœæ»ä¸å‰")
                print("   å»ºè®®: æ£€æŸ¥å¥–åŠ±å‡½æ•°è®¾è®¡ï¼Œè€ƒè™‘å¢åŠ å¥–åŠ±å¡‘å½¢")
            elif recent_trend < -0.05:
                print("âš ï¸  è­¦å‘Š: å¥–åŠ±ä¸‹é™")
                print("   å»ºè®®: é™ä½å­¦ä¹ ç‡ï¼Œæ£€æŸ¥æ˜¯å¦è¿‡æ‹Ÿåˆ")
            else:
                print("âœ“ å¥–åŠ±è¶‹åŠ¿æ­£å¸¸")
        
        # 2. æ£€æŸ¥ç­–ç•¥æŸå¤±
        policy_losses = metrics["policy_losses"]
        if len(policy_losses) > 10:
            if np.std(policy_losses[-10:]) > 1.0:
                print("\nâš ï¸  è­¦å‘Š: ç­–ç•¥æŸå¤±æ³¢åŠ¨å‰§çƒˆ")
                print("   å»ºè®®: å‡å°å­¦ä¹ ç‡ï¼Œå¢åŠ batch size")
            else:
                print("\nâœ“ ç­–ç•¥æŸå¤±ç¨³å®š")
        
        # 3. æ£€æŸ¥æˆåŠŸç‡
        success_rate = metrics["success_rate"]
        if len(success_rate) > 10:
            recent_success = np.mean(success_rate[-10:])
            if recent_success < 0.2:
                print("\nâš ï¸  è­¦å‘Š: æˆåŠŸç‡è¿‡ä½")
                print("   å»ºè®®: ç®€åŒ–ä»»åŠ¡ï¼Œä½¿ç”¨è¯¾ç¨‹å­¦ä¹ ")
            elif recent_success > 0.9:
                print("\nâœ“ æˆåŠŸç‡è‰¯å¥½ï¼Œå¯ä»¥å¢åŠ ä»»åŠ¡éš¾åº¦")
            else:
                print("\nâœ“ æˆåŠŸç‡æ­£å¸¸")
    
    @staticmethod
    def visualize_trajectory(trajectory: List[Tuple]):
        """å¯è§†åŒ–å•æ¡è½¨è¿¹"""
        print("\n=== è½¨è¿¹åˆ†æ ===\n")
        
        for i, (state, action, reward, info, _, value) in enumerate(trajectory):
            print(f"æ­¥éª¤ {i + 1}:")
            print(f"  åŠ¨ä½œç±»å‹: {action['type']}")
            print(f"  å¥–åŠ±: {reward:.3f}")
            print(f"  çŠ¶æ€ä»·å€¼ä¼°è®¡: {value.item():.3f}")
            
            if action['type'] == 'code':
                print(f"  ä»£ç é•¿åº¦: {len(action['content'])} å­—ç¬¦")
                # æ£€æŸ¥å¸¸è§é”™è¯¯æ¨¡å¼
                if 'import' not in action['content'] and i == 0:
                    print("  âš ï¸  å¯èƒ½ç¼ºå°‘å¿…è¦çš„å¯¼å…¥")
            
            print()
    
    @staticmethod
    def check_sandbox_safety(code: str) -> Tuple[bool, List[str]]:
        """æ£€æŸ¥ä»£ç å®‰å…¨æ€§"""
        warnings = []
        
        dangerous_patterns = [
            ('os.system', 'ç³»ç»Ÿå‘½ä»¤æ‰§è¡Œ'),
            ('subprocess.Popen', 'è¿›ç¨‹åˆ›å»º'),
            ('eval(', 'åŠ¨æ€ä»£ç æ‰§è¡Œ'),
            ('exec(', 'åŠ¨æ€ä»£ç æ‰§è¡Œ'),
            ('__import__', 'åŠ¨æ€å¯¼å…¥'),
            ('open(', 'æ–‡ä»¶æ“ä½œï¼ˆæ£€æŸ¥è·¯å¾„ï¼‰')
        ]
        
        for pattern, description in dangerous_patterns:
            if pattern in code:
                warnings.append(f"æ£€æµ‹åˆ° {pattern}: {description}")
        
        is_safe = len(warnings) == 0
        return is_safe, warnings
    
    @staticmethod
    def profile_execution_time(env: SimpleSandboxEnv, agent, task: str, num_runs: int = 10):
        """æ€§èƒ½åˆ†æ"""
        import time
        
        print(f"\n=== æ€§èƒ½åˆ†æ ({num_runs} æ¬¡è¿è¡Œ) ===\n")
        
        times = {
            'action_selection': [],
            'code_execution': [],
            'total': []
        }
        
        for _ in range(num_runs):
            state = env.reset(task)
            done = False
            
            run_start = time.time()
            
            while not done:
                # æµ‹é‡åŠ¨ä½œé€‰æ‹©æ—¶é—´
                action_start = time.time()
                action, _, _ = agent.select_action(state)
                times['action_selection'].append(time.time() - action_start)
                
                # æµ‹é‡æ‰§è¡Œæ—¶é—´
                exec_start = time.time()
                state, _, done, _ = env.step(action)
                times['code_execution'].append(time.time() - exec_start)
            
            times['total'].append(time.time() - run_start)
        
        print(f"åŠ¨ä½œé€‰æ‹©å¹³å‡æ—¶é—´: {np.mean(times['action_selection']):.3f}s")
        print(f"ä»£ç æ‰§è¡Œå¹³å‡æ—¶é—´: {np.mean(times['code_execution']):.3f}s")
        print(f"æ€»ä½“å¹³å‡æ—¶é—´: {np.mean(times['total']):.3f}s")
        
        return times

# ä½¿ç”¨è°ƒè¯•å·¥å…·
debugger = SandboxDebugger()

# è¯Šæ–­è®­ç»ƒ
debugger.diagnose_training_issues(metrics)

# åˆ†æè½¨è¿¹
sample_trajectory = trainer._collect_trajectory(math_tasks[0])
debugger.visualize_trajectory(sample_trajectory)

# å®‰å…¨æ£€æŸ¥
test_code = """
import numpy as np
result = np.sum([1, 2, 3, 4, 5])
print(result)
"""
is_safe, warnings = debugger.check_sandbox_safety(test_code)
print(f"\nä»£ç å®‰å…¨æ£€æŸ¥: {'é€šè¿‡' if is_safe else 'å¤±è´¥'}")
for warning in warnings:
    print(f"  - {warning}")

# æ€§èƒ½åˆ†æ
profile_results = debugger.profile_execution_time(
    env, agent, "è®¡ç®—1+1", num_runs=5
)
```

### å¯è§†åŒ–å­¦ä¹ è¿‡ç¨‹

```python
def visualize_learning_process(agent, env, task: str):
    """å¯è§†åŒ–æ™ºèƒ½ä½“çš„å­¦ä¹ è¿‡ç¨‹"""
    
    # è®°å½•ä¸åŒè®­ç»ƒé˜¶æ®µçš„è¡Œä¸º
    checkpoints = [0, 25, 50, 75, 100]  # å‡è®¾è®­ç»ƒäº†100è½®
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.flatten()
    
    for idx, checkpoint in enumerate(checkpoints):
        if idx >= len(axes):
            break
        
        # åŠ è½½å¯¹åº”checkpointçš„æ¨¡å‹ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
        state = env.reset(task)
        actions_taken = []
        rewards_received = []
        done = False
        
        while not done and len(actions_taken) < 10:
            action, _, _ = agent.select_action(state)
            actions_taken.append(action['type'])
            state, reward, done, _ = env.step(action)
            rewards_received.append(reward)
        
        # ç»˜åˆ¶è¯¥é˜¶æ®µçš„è¡Œä¸º
        ax = axes[idx]
        ax.bar(range(len(rewards_received)), rewards_received)
        ax.set_title(f'Iteration {checkpoint}')
        ax.set_xlabel('Step')
        ax.set_ylabel('Reward')
        ax.set_ylim([-0.5, 1.5])
        
        # æ ‡æ³¨åŠ¨ä½œç±»å‹
        for i, action_type in enumerate(actions_taken):
            color = {'code': 'blue', 'text': 'green', 'submit': 'red'}.get(action_type, 'gray')
            ax.axvline(i, color=color, alpha=0.3, linestyle='--')
    
    plt.tight_layout()
    plt.savefig('learning_process_visualization.png', dpi=300)
    print("å­¦ä¹ è¿‡ç¨‹å¯è§†åŒ–å·²ä¿å­˜")

# ç”Ÿæˆå¯è§†åŒ–
visualize_learning_process(agent, env, "è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬10é¡¹")
```

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

```python
class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å»ºè®®ç³»ç»Ÿ"""
    
    @staticmethod
    def optimize_inference():
        """æ¨ç†ä¼˜åŒ–å»ºè®®"""
        print("=== æ¨ç†æ€§èƒ½ä¼˜åŒ–å»ºè®® ===\n")
        
        print("1. æ¨¡å‹é‡åŒ–")
        print("   - ä½¿ç”¨8bité‡åŒ–å¯å‡å°‘50%å†…å­˜å ç”¨")
        print("   - ä»£ç ç¤ºä¾‹:")
        print("""
        from transformers import BitsAndBytesConfig
        
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config
        )
        """)
        
        print("\n2. æ‰¹å¤„ç†æ¨ç†")
        print("   - æ‰¹é‡å¤„ç†å¤šä¸ªä»»åŠ¡å¯æå‡30-40%ååé‡")
        print("   - ä»£ç ç¤ºä¾‹:")
        print("""
        def batch_inference(agent, tasks, batch_size=4):
            results = []
            for i in range(0, len(tasks), batch_size):
                batch = tasks[i:i+batch_size]
                # å¹¶è¡Œå¤„ç†batchä¸­çš„ä»»åŠ¡
                batch_results = process_batch(agent, batch)
                results.extend(batch_results)
            return results
        """)
        
        print("\n3. KVç¼“å­˜ä¼˜åŒ–")
        print("   - å¯ç”¨past_key_valuesç¼“å­˜å¯å‡å°‘é‡å¤è®¡ç®—")
        print("   - é€‚ç”¨äºå¤šè½®å¯¹è¯åœºæ™¯")
    
    @staticmethod
    def optimize_training():
        """è®­ç»ƒä¼˜åŒ–å»ºè®®"""
        print("\n=== è®­ç»ƒæ€§èƒ½ä¼˜åŒ–å»ºè®® ===\n")
        
        print("1. æ··åˆç²¾åº¦è®­ç»ƒ")
        print("   - ä½¿ç”¨FP16å¯åŠ é€Ÿ2-3å€")
        print("   - ä»£ç ç¤ºä¾‹:")
        print("""
        from torch.cuda.amp import autocast, GradScaler
        
        scaler = GradScaler()
        
        with autocast():
            outputs = model(**inputs)
            loss = compute_loss(outputs)
        
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        """)
        
        print("\n2. æ¢¯åº¦ç´¯ç§¯")
        print("   - åœ¨GPUå†…å­˜å—é™æ—¶æ¨¡æ‹Ÿå¤§batch size")
        print("   - ä»£ç ç¤ºä¾‹:")
        print("""
        accumulation_steps = 4
        
        for i, batch in enumerate(dataloader):
            loss = compute_loss(batch) / accumulation_steps
            loss.backward()
            
            if (i + 1) % accumulation_steps == 0:
                optimizer.step()
                optimizer.zero_grad()
        """)
        
        print("\n3. åˆ†å¸ƒå¼è®­ç»ƒ")
        print("   - ä½¿ç”¨å¤šGPUå¯çº¿æ€§åŠ é€Ÿè®­ç»ƒ")
        print("   - æ¨èä½¿ç”¨DeepSpeedæˆ–FSDP")
    
    @staticmethod
    def optimize_sandbox():
        """æ²™ç®±ä¼˜åŒ–å»ºè®®"""
        print("\n=== æ²™ç®±æ€§èƒ½ä¼˜åŒ–å»ºè®® ===\n")
        
        print("1. å®¹å™¨åŒ–éš”ç¦»")
        print("   - ä½¿ç”¨Dockerå®¹å™¨æä¾›æ›´å¥½çš„éš”ç¦»å’Œèµ„æºé™åˆ¶")
        print("   - ç¤ºä¾‹é…ç½®:")
        print("""
        docker run -it --rm \\
            --cpus=1.0 \\
            --memory=512m \\
            --network=none \\
            python:3.9 python script.py
        """)
        
        print("\n2. ä»£ç ç¼“å­˜")
        print("   - ç¼“å­˜å¸¸ç”¨ä»£ç ç‰‡æ®µçš„æ‰§è¡Œç»“æœ")
        print("   - å¯å‡å°‘é‡å¤æ‰§è¡Œå¼€é”€")
        
        print("\n3. å¼‚æ­¥æ‰§è¡Œ")
        print("   - ä½¿ç”¨å¼‚æ­¥I/Oå¤„ç†å¤šä¸ªæ²™ç®±å®ä¾‹")
        print("   - æå‡å¹¶å‘å¤„ç†èƒ½åŠ›")

# è¿è¡Œä¼˜åŒ–å»ºè®®
optimizer = PerformanceOptimizer()
optimizer.optimize_inference()
optimizer.optimize_training()
optimizer.optimize_sandbox()
```

## æ€»ç»“

### ç®—æ³•é€‚ç”¨åœºæ™¯

LLM-in-Sandbox-RLç‰¹åˆ«é€‚åˆä»¥ä¸‹åœºæ™¯ï¼š

1. **éœ€è¦å·¥å…·è°ƒç”¨çš„å¤æ‚ä»»åŠ¡**
   - æ•°å­¦è®¡ç®—ã€æ•°æ®åˆ†æã€æ–‡ä»¶å¤„ç†
   - éœ€è¦å¤–éƒ¨èµ„æºè®¿é—®ï¼ˆæ•°æ®åº“ã€APIç­‰ï¼‰

2. **å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡**
   - éœ€è¦ä¸­é—´ç»“æœéªŒè¯
   - é•¿æœŸè§„åˆ’å’Œæ‰§è¡Œ

3. **ä»£ç ç”Ÿæˆä¸æ‰§è¡Œ**
   - è‡ªåŠ¨åŒ–è„šæœ¬ç¼–å†™
   - æ•°æ®å¤„ç†æµç¨‹æ„å»º

4. **å—é™èµ„æºç¯å¢ƒ**
   - ä½¿ç”¨éæ™ºèƒ½ä½“æ•°æ®è®­ç»ƒæ™ºèƒ½ä½“è¡Œä¸º
   - é™ä½æ•°æ®æ”¶é›†æˆæœ¬

### ä¼˜ç¼ºç‚¹åˆ†æ

**ä¼˜ç‚¹**ï¼š
- âœ… æ— éœ€å¤§é‡æ™ºèƒ½ä½“è½¨è¿¹æ•°æ®å³å¯è®­ç»ƒ
- âœ… è‡ªç„¶æ”¯æŒå·¥å…·ä½¿ç”¨å’Œå¤–éƒ¨èµ„æºè®¿é—®
- âœ… å¯è§£é‡Šæ€§å¼ºï¼ˆå¯ä»¥æŸ¥çœ‹æ‰§è¡Œçš„ä»£ç ï¼‰
- âœ… å®‰å…¨æ€§å¯æ§ï¼ˆæ²™ç®±éš”ç¦»ï¼‰

**ç¼ºç‚¹**ï¼š
- âŒ æ²™ç®±æ‰§è¡Œæœ‰é¢å¤–å¼€é”€
- âŒ éœ€è¦ç²¾å¿ƒè®¾è®¡å¥–åŠ±å‡½æ•°
- âŒ è®­ç»ƒè¿‡ç¨‹å¯èƒ½ä¸ç¨³å®šï¼ˆRLå›ºæœ‰é—®é¢˜ï¼‰
- âŒ å¯¹åŸºç¡€æ¨¡å‹èƒ½åŠ›æœ‰ä¸€å®šè¦æ±‚

### è¿›é˜¶é˜…è¯»æ¨è

1. **å¼ºåŒ–å­¦ä¹ åŸºç¡€**
   - Sutton & Barto: "Reinforcement Learning: An Introduction"
   - Schulman et al.: "Proximal Policy Optimization Algorithms"

2. **è¯­è¨€æ¨¡å‹ä¸å·¥å…·ä½¿ç”¨**
   - Schick et al.: "Toolformer: Language Models Can Teach Themselves to Use Tools"
   - Nakano et al.: "WebGPT: Browser-assisted question-answering with human feedback"

3. **ä»£ç ç”Ÿæˆä¸æ‰§è¡Œ**
   - Chen et al.: "Evaluating Large Language Models Trained on Code"
   - Austin et al.: "Program Synthesis with Large Language Models"

4. **LLM-in-SandboxåŸè®ºæ–‡**
   - "LLM-in-Sandbox Elicits General Agentic Intelligence" (arXiv:2601.16206)

### å®è·µå»ºè®®

1. **ä»ç®€å•ä»»åŠ¡å¼€å§‹**ï¼šå…ˆåœ¨ç®€å•ç¯å¢ƒéªŒè¯ç®—æ³•ï¼Œå†é€æ­¥å¢åŠ å¤æ‚åº¦
2. **é‡è§†å¥–åŠ±è®¾è®¡**ï¼šç¨€ç–å¥–åŠ±å¾ˆéš¾å­¦ä¹ ï¼Œä½¿ç”¨å¥–åŠ±å¡‘å½¢
3. **ç›‘æ§è®­ç»ƒè¿‡ç¨‹**ï¼šå®šæœŸè¯„ä¼°ï¼ŒåŠæ—¶å‘ç°é—®é¢˜
4. **å®‰å…¨ç¬¬ä¸€**ï¼šä¸¥æ ¼é™åˆ¶æ²™ç®±æƒé™ï¼Œé¿å…æ¶æ„ä»£ç æ‰§è¡Œ
5. **å……åˆ†æµ‹è¯•**ï¼šåœ¨å¤šç§ä»»åŠ¡ä¸Šè¯„ä¼°æ³›åŒ–èƒ½åŠ›

é€šè¿‡æœ¬æ•™ç¨‹ï¼Œä½ åº”è¯¥å·²ç»æŒæ¡äº†LLM-in-Sandbox-RLçš„æ ¸å¿ƒåŸç†å’Œå®ç°æ–¹æ³•ã€‚è¿™æ˜¯ä¸€ä¸ªå‰æ²¿ä¸”å®ç”¨çš„æŠ€æœ¯æ–¹å‘ï¼Œå°†è¯­è¨€æ¨¡å‹çš„ç†è§£èƒ½åŠ›ä¸ä»£ç æ‰§è¡Œçš„ç²¾ç¡®æ€§ç»“åˆï¼Œä¸ºæ„å»ºæ›´å¼ºå¤§çš„AIæ™ºèƒ½ä½“æä¾›äº†æ–°æ€è·¯ã€‚